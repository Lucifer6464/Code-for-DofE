Explanation:

It is a classification algorithm
It is given a data point and attempts to classify this
datapoint with one on the classes it knows
It does this by looking at an unknown datapoint on a graph
of known datapoints and the most amount of datapoints that
it is near to is the one it will be classified

KNN stands for k-Nearest Neighbours
k is a hyper-parameter that stands for the amount of neighbours
that are to be looked for
If it was three then it would look at the three nearest
neighbours and choose what datapoint the unknown one should
be out of the three closest ones
If there is the same amount of points then it will break
so this means you always have to pick an odd number so
there is always a class that has more

Math:

It needs to know which point is closer so it has to do some
math to work this out

This method is called Euclidean Distance and is the default
way that it works out the distance and other methods are
quite similar

assign P1 to the unknown point with the two space of X1 and Y1
assign P2 to the known point with the two space of X1 and Y1
assign d to distance
     ________________________
d = / (X2 - X1)^ + (Y2 - Y1)^

With three dimensions the unknown point would have the three
space of X1, Y1 and Z1 and the known point would have the three
space of X2, Y2 and Z2 and the math would be
     _____________________________________
d = / (X2 - X1)^ + (Y2 - Y1)^ + (Z2 - Z1)^

Example:
Y2 is 4 and all other values are 0 so therefore the distance
is 0, see if the formula produces the same answer:
     ____________________
d = / (0 - 0)^ + (4 - 0)^

   _______
= / 0^ + 4^

   ____
= / 16

= 4

It is right


Note 1:

be careful choosing the right size for k because if there
are two datasets of 4 and 5 and k = 9 then even if the unknown
point is right next to the set of 4 it will still go
with the set of 5 because there are more points

Note 2:

computation on every datapoint is necessary to find out which
ones are closest but can be very slow